{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "intro_markdown"
   },
   "source": [
    "# Project 4: Heart Disease Prediction (Classification) ðŸ©º\n",
    "\n",
    "**Project Objective:** To build a machine learning model that can accurately predict whether a patient has heart disease based on a set of medical attributes. This project will serve as a comprehensive introduction to classification, one of the most common types of machine learning problems.\n",
    "\n",
    "\n",
    "### Core Concepts We'll Cover:\n",
    "1.  **Classification Fundamentals:** Understanding the goal of predicting a discrete category.\n",
    "2.  **Exploratory Data Analysis (EDA) for Classification:** Analyzing features to find patterns that distinguish between classes.\n",
    "3.  **Data Preprocessing:** Preparing data for classification models using encoding and feature scaling.\n",
    "4.  **Model Building:** Training and comparing a simple baseline model (Logistic Regression) with an advanced ensemble model (Random Forest).\n",
    "5.  **Model Evaluation:** Mastering key classification metrics like Accuracy, Precision, Recall, F1-Score, and interpreting the Confusion Matrix.\n",
    "6.  **Feature Importance:** Identifying the most influential medical factors for predicting heart disease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "theory_classification"
   },
   "source": [
    "### **Theoretical Concept: What is Classification?**\n",
    "\n",
    "Classification is a type of supervised machine learning task where the goal is to predict a **discrete category or class label**. This is different from regression, where we predict a continuous numerical value.\n",
    "\n",
    "**Classification vs. Regression:**\n",
    "- **Classification:** Is this email spam or not spam? (Two classes)\n",
    "- **Regression:** What will be the price of this house? (Continuous value)\n",
    "\n",
    "In this project, our goal is to predict one of two classes for a patient: `0` (No Heart Disease) or `1` (Has Heart Disease). This is a **binary classification** problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_markdown"
   },
   "source": [
    "### Step 1: Setup - Importing Libraries and Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "setup_code"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kagglehub'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkagglehub\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler, OneHotEncoder\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'kagglehub'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import kagglehub\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_score, recall_score, f1_score\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 276
    },
    "id": "load_data_code",
    "outputId": "25101a5e-dae2-4a32-c7d9-ec604d966cd5"
   },
   "outputs": [],
   "source": [
    "# Download the dataset using the Kaggle Hub API\n",
    "print(\"Downloading dataset...\")\n",
    "path = kagglehub.dataset_download(\"redwankarimsony/heart-disease-data\")\n",
    "\n",
    "# Load the dataset from the downloaded path\n",
    "file_path = f'{path}/heart_disease_uci.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(\"Dataset downloaded and loaded successfully.\")\n",
    "print(f\"Data shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eda_markdown"
   },
   "source": [
    "### Step 2: Exploratory Data Analysis (EDA)\n",
    "Before building any models, we need to understand our data deeply. We'll look at the distribution of our target variable, the characteristics of our features, and how they relate to the presence of heart disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "initial_inspection_code",
    "outputId": "4ca5a903-67c8-4068-f4b9-54b6c84c7b59"
   },
   "outputs": [],
   "source": [
    "# Initial inspection\n",
    "print(\"Dataset Information:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nDescriptive Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 586
    },
    "id": "c89E71xZgWnD",
    "outputId": "aaee826d-8f35-4c31-8536-75195a481956"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "target_analysis_markdown"
   },
   "source": [
    "#### 2.1 Analyzing the Target Variable\n",
    "Let's see the distribution of patients with and without heart disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "target_analysis_code",
    "outputId": "2d84e9a0-5fac-4c32-a0d1-457b76a5ad7f"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='num', data=df, palette='viridis', hue='num', legend=False)\n",
    "plt.title('Distribution of Heart Disease (1 = Disease, 0 = No Disease)')\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "target_analysis_summary"
   },
   "source": [
    "**Insight:** The dataset is fairly balanced, with a slightly higher number of patients having heart disease. This is good because it means our model will have a similar number of examples for both classes to learn from, and accuracy will be a meaningful metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feature_analysis_markdown"
   },
   "source": [
    "#### 2.2 Analyzing Features vs. Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "feature_analysis_code",
    "outputId": "00050dd7-a468-4c98-ca7f-a024c848ad21"
   },
   "outputs": [],
   "source": [
    "# Let's visualize the relationship between key features and the target\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "fig.suptitle('Key Features vs. Heart Disease', fontsize=16)\n",
    "\n",
    "# Age vs. Target\n",
    "sns.histplot(ax=axes[0, 0], data=df, x='age', hue='num', multiple='stack', palette='plasma').set_title('Age Distribution by Target')\n",
    "\n",
    "# Max Heart Rate vs. Target\n",
    "sns.boxplot(ax=axes[0, 1], data=df, x='num', y='thalch', palette='magma', hue='num', legend=False).set_title('Max Heart Rate by Target')\n",
    "\n",
    "# Chest Pain Type vs. Target\n",
    "cp_plot = sns.countplot(ax=axes[1, 0], data=df, x='cp', hue='num', palette='cividis')\n",
    "cp_plot.set_title('Chest Pain Type by Target')\n",
    "cp_plot.set_xticks(range(len(df['cp'].unique())))\n",
    "cp_plot.set_xticklabels(['Typical Angina', 'Atypical Angina', 'Non-anginal Pain', 'Asymptomatic'])\n",
    "\n",
    "# Sex vs. Target\n",
    "sex_plot = sns.countplot(ax=axes[1, 1], data=df, x='sex', hue='num', palette='inferno')\n",
    "sex_plot.set_title('Sex by Target')\n",
    "sex_plot.set_xticks(range(len(df['sex'].unique())))\n",
    "sex_plot.set_xticklabels(['Female', 'Male'])\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feature_analysis_summary"
   },
   "source": [
    "**Insights:**\n",
    "- **Max Heart Rate (`thalach`):** Patients with heart disease tend to have a lower maximum heart rate.\n",
    "- **Chest Pain (`cp`):** Patients with chest pain types 1 and 2 (Atypical and Non-anginal) are more likely to have heart disease. Surprisingly, those with type 0 (Typical Angina) are less likely, and those with asymptomatic pain (type 3) are very likely to have the disease.\n",
    "- **Sex:** A higher proportion of females in this dataset have heart disease compared to males."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "correlation_code",
    "outputId": "d4592b1e-3d14-4230-cac4-97b2bb88447f"
   },
   "outputs": [],
   "source": [
    "# Correlation Heatmap\n",
    "plt.figure(figsize=(16, 12))\n",
    "# Select only numerical columns for correlation calculation\n",
    "numerical_df = df.select_dtypes(include=np.number)\n",
    "sns.heatmap(numerical_df.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix of Numerical Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "preprocessing_markdown"
   },
   "source": [
    "### Step 3: Data Preprocessing\n",
    "Even though the data is clean, we need to prepare it for our models. This involves:\n",
    "1.  **Separating features (X) and target (y).**\n",
    "2.  **Identifying categorical features** that need to be encoded.\n",
    "3.  **One-Hot Encoding** categorical features to convert them into a numerical format.\n",
    "4.  **Scaling numerical features** so they are on a similar scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjVVXv-veLpM"
   },
   "source": [
    "## **Theoretical Concept: Scikit-Learn Pipelines**\n",
    "\n",
    "A **Pipeline** in Scikit-Learn is a way to automate a machine learning workflow. It allows you to chain together multiple steps, such as preprocessing, dimensionality reduction, and model training, into a single object.\n",
    "\n",
    "**Why use Pipelines?**\n",
    "\n",
    "1.  **Convenience:** Simplifies the code and makes the workflow easier to manage.\n",
    "2.  **Prevents Data Leakage:** Ensures that data preprocessing steps learned from the training data are applied only to the training data, and the same transformations are then applied to the test data *after* the split. This prevents information from the test set from \"leaking\" into the training process.\n",
    "3.  **Cleaner Code:** Organizes steps logically, making the code more readable and maintainable.\n",
    "4.  **Simplified Hyperparameter Tuning:** Makes it easier to tune hyperparameters for all steps in the pipeline using techniques like cross-validation.\n",
    "\n",
    "In this project, we'll use a pipeline to combine our preprocessing steps (imputation, scaling, and one-hot encoding) with our classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "split_data_code"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df.drop('num', axis=1)\n",
    "y = df['num']\n",
    "\n",
    "# Drop the 'id' and 'dataset' columns as they are not features\n",
    "X = X.drop(['id', 'dataset'], axis=1)\n",
    "\n",
    "\n",
    "# Identify categorical and numerical features\n",
    "categorical_features = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'thal']\n",
    "numerical_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'ca']\n",
    "\n",
    "# Create preprocessing pipelines for numerical and categorical features\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')), # Added imputation for categorical features\n",
    "    ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Create a column transformer to apply different transformations to different columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rb_mK8CvEWaA"
   },
   "source": [
    "* Create numerical preprocessing pipeline: A Pipeline is created to handle numerical features. It first uses SimpleImputer with the strategy 'mean' to fill in missing numerical values with the mean of the column, and then uses StandardScaler to scale the numerical features to have zero mean and unit variance.\n",
    "* Create categorical preprocessing pipeline: A Pipeline is created for categorical features. It uses SimpleImputer with the strategy 'most_frequent' to fill in missing categorical values with the most frequent value, and then applies OneHotEncoder to convert categorical variables into a numerical format. drop='first' is used to avoid multicollinearity, and handle_unknown='ignore' allows the model to handle unseen categories during testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "modeling_markdown"
   },
   "source": [
    "### Step 4: Model Building & Training\n",
    "We will build two models and wrap them in a Scikit-Learn Pipeline. The pipeline will automatically apply our preprocessing steps to the data before training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VjWE6f1eIabA"
   },
   "source": [
    "#### **Theoretical Concept: Classification Models**\n",
    "\n",
    "Let's dive into more detail on the classification models we are using:\n",
    "\n",
    "*   **Logistic Regression:**\n",
    "    Logistic Regression is a **linear classification algorithm** used for binary classification problems (though it can be extended for multiclass). Despite the name \"regression,\" it's a classification method. It works by using a **sigmoid (or logistic) function** to map the output of a linear equation (`wTx + b`) to a probability value between 0 and 1. This probability represents the likelihood that a given data point belongs to a specific class (e.g., the positive class). A threshold (commonly 0.5) is then applied to these probabilities to assign the class label. The model learns the optimal weights (`w`) and bias (`b`) that define a linear decision boundary to separate the classes.\n",
    "\n",
    "*   **Random Forest:**\n",
    "    Random Forest is an **ensemble learning method** that belongs to the tree-based models. It builds a large number of **decision trees** during training. Each tree is trained on a **random subset** of the training data (bootstrapping) and considers only a **random subset** of features at each split point. For classification, the final prediction is made by taking a **majority vote** of the predictions from all individual trees. This randomness in building trees helps to reduce **variance** and prevent **overfitting**, making Random Forests more robust and generally higher performing than a single decision tree.\n",
    "\n",
    "*   **Support Vector Machine (SVM):**\n",
    "    Support Vector Machine is a powerful algorithm that can be used for both linear and non-linear classification. The fundamental idea behind SVM is to find the **optimal hyperplane** that separates the data points of different classes in a high-dimensional space. The \"optimal\" hyperplane is the one that has the **largest margin** between the closest data points of the different classes (these points are called **support vectors**). For non-linearly separable data, SVM uses the **kernel trick** to implicitly map the data into a higher-dimensional feature space where a linear separation might be possible. Common kernels include the linear kernel, polynomial kernel, and Radial Basis Function (RBF) kernel.\n",
    "\n",
    "*   **K-Nearest Neighbors (KNN):**\n",
    "    K-Nearest Neighbors is a simple and intuitive **instance-based** or **lazy learning** algorithm. It doesn't learn a discriminative function from the training data during a training phase. Instead, it memorizes the training dataset. To classify a new, unseen data point, it calculates the **distance** (e.g., Euclidean distance) between this new point and all points in the training dataset. It then identifies the **'k' nearest data points**. The class label assigned to the new point is determined by the **majority class** among these 'k' nearest neighbors. The choice of 'k' and the distance metric are important hyperparameters that can significantly affect performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_lr_markdown"
   },
   "source": [
    "#### 4.1 Model 1: Logistic Regression (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lr_code",
    "outputId": "19c430f6-b463-463d-ce79-b27280314324"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Identify categorical and numerical features directly from X_train columns\n",
    "all_features = X_train.columns.tolist()\n",
    "categorical_features = [col for col in all_features if X_train[col].dtype == 'object']\n",
    "numerical_features = [col for col in all_features if X_train[col].dtype != 'object']\n",
    "\n",
    "print(\"Numerical features:\", numerical_features)\n",
    "print(\"Categorical features:\", categorical_features)\n",
    "\n",
    "\n",
    "# Create preprocessing pipelines for numerical and categorical features\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Create a column transformer to apply different transformations to different columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Create the Logistic Regression pipeline\n",
    "lr_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('classifier', LogisticRegression(random_state=42))])\n",
    "\n",
    "lr_pipeline.fit(X_train, y_train)\n",
    "y_pred_lr = lr_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_rf_markdown"
   },
   "source": [
    "#### 4.2 Model 2: Random Forest Classifier (Advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rf_code"
   },
   "outputs": [],
   "source": [
    "rf_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))])\n",
    "\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "y_pred_rf = rf_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "940cc19d"
   },
   "source": [
    "#### 4.3 Model 3: Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3b545200"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create the SVM pipeline\n",
    "svm_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('classifier', SVC(random_state=42))])\n",
    "\n",
    "svm_pipeline.fit(X_train, y_train)\n",
    "y_pred_svm = svm_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dbd69b78"
   },
   "source": [
    "#### 4.4 Model 4: K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "46c965d8"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create the KNN pipeline\n",
    "knn_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('classifier', KNeighborsClassifier())])\n",
    "\n",
    "knn_pipeline.fit(X_train, y_train)\n",
    "y_pred_knn = knn_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation_markdown"
   },
   "source": [
    "### Step 5: Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "theory_metrics"
   },
   "source": [
    "#### **Theoretical Concept: The Confusion Matrix & Key Metrics**\n",
    "For classification, accuracy isn't the whole story. We use a **Confusion Matrix** to get a deeper look at performance.\n",
    "\n",
    "- **True Positives (TP):** Correctly predicted positive class (Model said 'Disease', patient has it).\n",
    "- **True Negatives (TN):** Correctly predicted negative class (Model said 'No Disease', patient doesn't have it).\n",
    "- **False Positives (FP):** Incorrectly predicted positive class (Model said 'Disease', but patient doesn't have it). Also called a **Type I Error**.\n",
    "- **False Negatives (FN):** Incorrectly predicted negative class (Model said 'No Disease', but patient has it). Also called a **Type II Error**. This is often the most dangerous type of error in medical diagnoses.\n",
    "\n",
    "\n",
    "\n",
    "From this, we derive key metrics:\n",
    "- **Accuracy:** (TP+TN) / Total. Overall, how often is the classifier correct?\n",
    "- **Precision:** TP / (TP+FP). Of all patients the model *predicted* would have the disease, how many actually did? (Measures the cost of FPs).\n",
    "- **Recall (Sensitivity):** TP / (TP+FN). Of all the patients who *actually* had the disease, how many did the model correctly identify? (Measures the cost of FNs).\n",
    "- **F1-Score:** The harmonic mean of Precision and Recall. It's a great single metric for evaluating a model's overall performance when there's a trade-off between Precision and Recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "evaluation_code",
    "outputId": "89c02b07-96a1-45a8-db29-7f3bf5023b71"
   },
   "outputs": [],
   "source": [
    "print(\"--- Logistic Regression Performance ---\")\n",
    "print(classification_report(y_test, y_pred_lr, zero_division=0))\n",
    "\n",
    "print(\"\\n--- Random Forest Performance ---\")\n",
    "print(classification_report(y_test, y_pred_rf, zero_division=0))\n",
    "\n",
    "print(\"\\n--- Support Vector Machine (SVM) Performance ---\")\n",
    "print(classification_report(y_test, y_pred_svm, zero_division=0))\n",
    "\n",
    "print(\"\\n--- K-Nearest Neighbors (KNN) Performance ---\")\n",
    "print(classification_report(y_test, y_pred_knn, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f921e862"
   },
   "source": [
    "### Step 7: Conclusion\n",
    "\n",
    "In this project, we built classification models for predicting heart disease.\n",
    "\n",
    "**Key Steps Undertaken:**\n",
    "1.  **Established the goal of classification:** Predicting a binary outcome (disease or no disease).\n",
    "2.  **Performed a thorough EDA:** Identified key medical indicators like chest pain type, max heart rate, and `ca` that are strongly related to the target.\n",
    "3.  **Built a robust preprocessing pipeline:** Handled categorical and numerical features systematically using `ColumnTransformer` and `Pipeline`.\n",
    "4.  **Trained and compared four models:** Evaluated Logistic Regression, Random Forest, Support Vector Machine (SVM), and K-Nearest Neighbors (KNN). The evaluation showed that the Support Vector Machine (SVM) performed slightly better than the other models in this analysis.\n",
    "5.  **Evaluated models with proper metrics:** Used the confusion matrix, precision, and recall to understand the model's performance in a medical context, where minimizing false negatives is critical.\n",
    "6.  **Interpreted model results:** Used feature importance (from the Random Forest model as an example) to confirm some of the most predictive medical factors, providing actionable insights.\n",
    "\n",
    "This end-to-end workflow demonstrates the application of classification in a real-world healthcare scenario, moving from raw data to predictive models and their evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73663d3f"
   },
   "source": [
    "**Evaluation Insight:** The Support Vector Machine (SVM) Classifier performs slightly better than the other models, achieving an overall accuracy of 0.59. While all models struggle with the less frequent classes (2, 3, and 4), SVM shows a slightly better F1-score for predicting class 1 (Heart Disease). The confusion matrix provided was for the Random Forest model, which showed good performance on classes 0 and 1 but also struggled with the less frequent classes. Based on the classification reports, SVM is the best performing model among the four in this evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "confusion_matrix_code",
    "outputId": "c377d3c6-ff2c-4e55-81fa-cf597fbc0c51"
   },
   "outputs": [],
   "source": [
    "# Visualize the confusion matrix for the best model (SVM)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_svm)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['No Disease', 'Disease', 'Severity 2', 'Severity 3', 'Severity 4'], yticklabels=['No Disease', 'Disease', 'Severity 2', 'Severity 3', 'Severity 4'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - Support Vector Machine (SVM)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4004d05"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7ec24e1"
   },
   "source": [
    "**Insight:** This feature importance analysis, derived from the Random Forest model, shows that `ca` (number of major vessels colored by flourosopy), `thalach` (max heart rate), `thal` (thalassemia type), and `cp` (chest pain type) are among the most important predictors. This aligns with our EDA and medical intuition, confirming that these factors are critical for diagnosing heart disease. This is provided as an example of feature importance, even though the SVM model performed slightly better overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation_summary"
   },
   "source": [
    "**Evaluation Insight:** The Random Forest Classifier performs exceptionally well, achieving near-perfect scores across the board (Accuracy, Precision, Recall, and F1-Score are all 99-100%). It significantly outperforms the Logistic Regression model. The confusion matrix shows it made only one error on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feature_importance_markdown"
   },
   "source": [
    "### Step 6: Feature Importance\n",
    "A major advantage of tree-based models like Random Forest is that we can easily see which features were most influential in making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "feature_importance_code"
   },
   "outputs": [],
   "source": [
    "# Extract feature names after one-hot encoding\n",
    "feature_names = rf_pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
    "\n",
    "# Get feature importances from the trained model\n",
    "importances = rf_pipeline.named_steps['classifier'].feature_importances_\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False).head(10)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance_df, palette='rocket', hue='Feature', legend=False)\n",
    "plt.title('Top 10 Most Important Features - Random Forest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feature_importance_summary"
   },
   "source": [
    "**Insight:** The model found that `ca` (number of major vessels colored by flourosopy), `thalach` (max heart rate), `thal` (thalassemia type), and `cp` (chest pain type) are among the most important predictors. This aligns with our EDA and medical intuition, confirming that these factors are critical for diagnosing heart disease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion_markdown"
   },
   "source": [
    "### Step 7: Conclusion\n",
    "\n",
    "In this project, we built a highly accurate classification model for predicting heart disease.\n",
    "\n",
    "**Key Steps Undertaken:**\n",
    "1.  **Established the goal of classification:** Predicting a binary outcome (disease or no disease).\n",
    "2.  **Performed a thorough EDA:** Identified key medical indicators like chest pain type, max heart rate, and `ca` that are strongly related to the target.\n",
    "3.  **Built a robust preprocessing pipeline:** Handled categorical and numerical features systematically using `ColumnTransformer` and `Pipeline`.\n",
    "4.  **Trained and compared two models:** Showed that the Random Forest Classifier (99% accuracy) was far superior to the Logistic Regression baseline (86% accuracy).\n",
    "5.  **Evaluated models with proper metrics:** Used the confusion matrix, precision, and recall to understand the model's performance in a medical context, where minimizing false negatives is critical.\n",
    "6.  **Interpreted model results:** Used feature importance to confirm the most predictive medical factors, providing actionable insights.\n",
    "\n",
    "This end-to-end workflow demonstrates the power of classification in a real-world healthcare scenario, moving from raw data to a highly accurate and interpretable predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wfy6DV6Xn7rJ"
   },
   "source": [
    "### Submission Criteria\n",
    "\n",
    "To fulfill the submission requirements for this project, please ensure the following:\n",
    "\n",
    "1.  **Complete Exploratory Data Analysis (EDA):** Perform all the necessary steps for analyzing the dataset, including visualizations and summaries to understand the data characteristics and relationships.\n",
    "2.  **Model Training without Pipelines:** Train at least one classification model directly, without using the Scikit-Learn `Pipeline` object for preprocessing and model chaining. This involves manually applying preprocessing steps (like imputation and scaling/encoding) to the data before training the model.\n",
    "3.  **Submit the Entire Notebook:** Ensure that the final submission includes the complete Colab notebook with all code cells executed and outputs visible.\n",
    "\n",
    "Meeting these criteria will demonstrate your understanding of the individual steps involved in a machine learning workflow."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
